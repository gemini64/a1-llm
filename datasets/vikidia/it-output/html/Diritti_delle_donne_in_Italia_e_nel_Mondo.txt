I diritti delle donne in Italia
Le giovani generazioni di donne italiane sono state cresciute con una serie di diritti, ma quando escono dalle famiglie e si approcciano con il mondo del lavoro o con la formazione del proprio nucleo familiare, riescono a capire solo in quel momento che ancora oggi nella società moderna molteplici sono le discriminazioni di genere. Le donne Italiane sono attualmente libere a livello economico, sociale giuridico e politico, ma questi diritti sono stati ottenuti con tanta fatica e con numerose battaglie. Infatti, per molto tempo, le donne sono state considerate esseri deboli e inferiori, emarginate dalla società, destinate al silenzio, all'invisibilità e alla riverenza all'uomo caput mundi. Solo nella prima metà del novecento ottengono risultati positivi sia nell'ambito del lavoro sia in quello dei diritti.