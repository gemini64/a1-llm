The probability of an event is a way of speaking about how likely this event is to happen.
If the sky is cloudy, then it may rain soon, but that is not certain. By observing clues like the color and shape of the clouds, we can decide that a storm is coming and that rain is likely to happen. On the other hand, if the clouds are little and sparse, rain is less likely. In both cases, the probability of rain is not the same: faced with a storm, we might say that we are “almost certain” that it will rain, whereas we “would be surprised” in the other case.
When the event spoken of is simple, probability is often expressed as a number between 0 and 1:

0 means that it is impossible for the event to happen;
1 means that it is certain that the event will happen;
0.5 means that it is as likely for the event to happen as for it not to happen.

The closer the number to 1 or 0, the more probable or improbable the corresponding event is: an event of probability 0.6 is more likely to happen than an event of probability 0.4.
If we hide 5 balls in a bag, all of them black but one white, what is the probability of picking the white one by chance? If we say that each ball is as likely to be picked as any other, then that probability is 1 out of 5, which means 1 ÷ 5 = 0.2, “not very likely.” On the contrary, the probability of picking a black ball is 4 out of 5, which means 4 ÷ 5 = 0.8, “quite likely.”
Still, probabilities are not unavoidable predictions: it is possible, though very improbable, that someone always picks the white ball, or never picks it. As the number of experiments increase, however, what happens in reality will match more and more closely the probability: after 1000 experiments, the actual number of times the white ball has been picked should be close to 200 (= 0.2 × 1000), even though we can't be as precise after only 10 experiments. This is called the law of large numbers.